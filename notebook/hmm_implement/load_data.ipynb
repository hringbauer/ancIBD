{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class that implements Emission Matrix for IBD detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute-e-16-231.o2.rc.hms.harvard.edu\n",
      "HSM O2 Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/hapBLOCK\n",
      "CPU Count: 28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import socket as socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import h5py\n",
    "import allel\n",
    "import itertools as it\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM O2 Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/hapBLOCK/\"  # The Path on Harvard Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class containing functions to load data needed for HMM IBD Run.\n",
    "Returns all relevant data for HMM in standardized format:\n",
    "1) Haplotype Probabilities (for ancestral allele)\n",
    "2) Allele Frequencies \n",
    "3) Genetic Map\n",
    "@ Author: Harald Ringbauer, September 2020\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import os as os\n",
    "import h5py as h5py\n",
    "\n",
    "###############################\n",
    "###############################\n",
    "\n",
    "class LoadData(object):\n",
    "    \"\"\"Class to load data in uniform format\"\"\"\n",
    "    path = \"\" # Path to load from (usually folder)\n",
    "    output = True # Whether to print output\n",
    "    \n",
    "    def __init__(self, path=\"\"):\n",
    "        \"\"\"Can remember path if given\"\"\"\n",
    "        self.path=path\n",
    "    \n",
    "    def return_p(self, **kwargs):\n",
    "        \"\"\"Return array of Allele Frequencies [l]\"\"\"\n",
    "        raise NotImplementedError(\"Implement This in specific subclass.\")\n",
    "\n",
    "    def return_map(self, **kwargs):\n",
    "        \"\"\"Return genetic map [l] in Morgan\"\"\"\n",
    "        raise NotImplementedError(\"Implement This in specific subclass.\")\n",
    "        \n",
    "    def return_haplotypes_ll(self, **kwargs):\n",
    "        \"\"\"Return haplotype likelihoods [4,l,2]\"\"\"\n",
    "        raise NotImplementedError(\"Implement This in specific subclass.\")\n",
    "        \n",
    "    def load_all_data(self, **kwargs):\n",
    "        \"\"\"Load all haplotype likelihoods\n",
    "        haplotype likelihoods [2*n,l,2]\n",
    "        derived allele frequencies [l]\n",
    "        map in Morgan [l]\"\"\"\n",
    "        htsl = self.return_haplotypes_ll()\n",
    "        p = self.return_p()\n",
    "        m = self.return_map()\n",
    "        self.check_valid_data(htsl, p, m)\n",
    "        return htsl, p, m\n",
    "    \n",
    "    def check_valid_data(self, htsl, p, m):\n",
    "        \"\"\"Check whether data in valid format\"\"\"\n",
    "        assert(np.shape(htsl)[1]==len(p))\n",
    "        assert(len(p)==len(m))\n",
    "        pass \n",
    "        \n",
    "    def set_params(self, **kwargs):\n",
    "        \"\"\"Set the Parameters.\n",
    "        Takes keyworded arguments\"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "###############################\n",
    "### Class for Simulated date\n",
    "\n",
    "class LoadSimulated(LoadData):\n",
    "    \"\"\"Class to load simulated data saved in standard format.\"\"\"\n",
    "    r_gap = 1.0\n",
    "    r_path = \"\"\n",
    "    \n",
    "    def load_all_data(self, **kwargs):\n",
    "        ### Calculate Paths\n",
    "        haplo_path = os.path.join(self.path, \"haplo_ll.tsv\")\n",
    "        p_path = os.path.join(self.path, \"p.tsv\")\n",
    "        \n",
    "        ### Load the Data\n",
    "        #htsl = pickle.load(open(haplo_path, \"rb\")) \n",
    "        htsl = np.loadtxt(haplo_path, delimiter=\"\\t\", dtype=\"float\")\n",
    "        p = np.loadtxt(p_path, delimiter=\"\\t\", dtype=\"float\")\n",
    "        if len(self.r_path)==0:\n",
    "            #m = np.ones(len(p), dtype=\"float\") * self.r_gap\n",
    "            m = np.arange(len(p), dtype=\"float\") * self.r_gap # Load absolute Positions\n",
    "        \n",
    "        self.check_valid_data(htsl, p, m)\n",
    "        return htsl, p, m\n",
    "    \n",
    "###############################\n",
    "### Class for HDF5 data\n",
    "\n",
    "class LoadHDF5(LoadData):\n",
    "    \"\"\"Class to load HDF data for 2 individuals in standard format.\"\"\"\n",
    "    path_h5=\"\"\n",
    "    iids = []\n",
    "    ch = 3   # Which chromosome to load\n",
    "    min_error=1e-5 # Minimum Probability of genotyping erro\n",
    "    \n",
    "    def return_map(self, f):\n",
    "        \"\"\"Return the recombination map\"\"\"\n",
    "        m = f[\"variants/MAP\"][:]\n",
    "        return m\n",
    "    \n",
    "    def return_p(self, f):\n",
    "        \"\"\"Return array of Allele Frequencies [l]\n",
    "        TODO: IMPLEMENT PROPER ALLELE FREQUENCY\"\"\"\n",
    "        #p = 0.5 * np.ones(len(f[\"variants/MAP\"]))\n",
    "        p = f[\"variants/AF_ALL\"][:] # Load the Allele Freqs from HDF5\n",
    "        return p\n",
    "        \n",
    "    def get_individual_idx(self, f, iid=\"\", f_col=\"samples\"):\n",
    "        \"\"\"Return index of individual iid\"\"\"\n",
    "        samples = f[f_col].asstr()[:]\n",
    "        idx = (samples == iid)\n",
    "        assert(np.sum(idx)==1) # Sanity Check\n",
    "        idx=np.where(idx)[0][0]\n",
    "        return idx  \n",
    "    \n",
    "    def get_haplo_prob(self, f, idx):\n",
    "        \"\"\"Get haploid ancestral probability for indivual [l,2]\"\"\"\n",
    "        h1 = f[\"calldata/GT\"][:,idx,:].T\n",
    "        m = np.max(f[\"calldata/GP\"][:,idx,:], axis=1)\n",
    "        m = np.minimum(m,  1 - self.min_error)\n",
    "        h1 = (1-h1) * m + h1 * (1 - m) # Probability of being ancestral\n",
    "        return h1\n",
    "        \n",
    "    def load_all_data(self, **kwargs):\n",
    "        \"\"\" Return haplotype likelihoods [2,l,2] for anc. allele\n",
    "        derived allele frequencies [l]\n",
    "        map in Morgan [l]\"\"\"\n",
    "        path_h5_ch = f\"{self.path}{self.ch}.h5\"\n",
    "        with h5py.File(path_h5_ch, \"r\") as f:\n",
    "            m = self.return_map(f)\n",
    "            p = self.return_p(f)\n",
    "            \n",
    "            idx1 = self.get_individual_idx(f, self.iids[0])\n",
    "            idx2 = self.get_individual_idx(f, self.iids[1])\n",
    "            h1 = self.get_haplo_prob(f, idx1)\n",
    "            h2 = self.get_haplo_prob(f, idx2)\n",
    "            htsl = np.concatenate((h1,h2), axis=0)\n",
    "        \n",
    "        self.check_valid_data(htsl, p, m)\n",
    "        return htsl, p, m\n",
    "    \n",
    "class LoadHDF5Multi(LoadHDF5):\n",
    "    \"\"\"Class to load HDF5 data for multiple individuals\"\"\"\n",
    "    path_h5=\"\"\n",
    "    iids = []\n",
    "    ch = 3   # Which chromosome to load\n",
    "    min_error=1e-5 # Minimum Probability of genotyping erro  \n",
    "    \n",
    "    def get_haplo_prob(self, f, idcs):\n",
    "        \"\"\"Get haploid ancestral probability for n individuals \n",
    "        Return [n,l,2] array\"\"\"\n",
    "        h1 = f[\"calldata/GT\"][:,idcs,:]\n",
    "        m = np.max(f[\"calldata/GP\"][:,idcs,:], axis=2)\n",
    "        m = np.minimum(m,  1 - self.min_error) # Max Cap of certainty\n",
    "        h1 = (1-h1) * m[:,:,None] + h1 * (1 - m[:,:,None]) # Probability of being ancestral\n",
    "        h1 = np.swapaxes(h1, 0, 1)\n",
    "        return h1\n",
    "    \n",
    "    def load_all_data(self, **kwargs):\n",
    "        \"\"\" Return haplotype likelihoods [n,l,2] for anc. allele\n",
    "        derived allele frequencies [l]\n",
    "        map in Morgan [l]\"\"\"\n",
    "        path_h5_ch = f\"{self.path}{self.ch}.h5\"\n",
    "        with h5py.File(path_h5_ch, \"r\") as f:\n",
    "            m = self.return_map(f)\n",
    "            #p = self.return_p(f)\n",
    "            \n",
    "            idcs = [self.get_individual_idx(f, iid) for iid in self.iids] \n",
    "            hts = self.get_haplo_prob(f, idcs)\n",
    "            htsl = np.concatenate(hts, axis=0)\n",
    "        \n",
    "        p = self.get_p(htsl)  # Calculate Mean allele frequency from subset\n",
    "        self.check_valid_data(htsl, p, m)\n",
    "        return htsl, p, m\n",
    "    \n",
    "    def get_p(self, htsl):\n",
    "        \"\"\"Get Allele frequency from haplotype probabilities\"\"\"\n",
    "        p_anc = np.mean(htsl, axis=(0, 2))\n",
    "        p_der = 1 - p_anc # get the derived allele frequency\n",
    "        return p_der\n",
    "    \n",
    "###############################    \n",
    "###############################\n",
    "### Factory Method\n",
    "    \n",
    "def load_loaddata(l_model=\"simulated\", path=\"\", **kwargs):\n",
    "    \"\"\"Factory method to return the right loading Model\"\"\"\n",
    "    if l_model == \"simulated\":\n",
    "        l_obj = LoadSimulated(path=path, **kwargs)\n",
    "    elif l_model == \"hdf5\":\n",
    "        l_obj = LoadHDF5(path=path, **kwargs)\n",
    "    elif l_model == \"hdf5multi\":\n",
    "        l_obj = LoadHDF5Multi(path=path, **kwargs)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Loading Model not found!\")\n",
    "    return l_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested whether loading worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_loaddata(l_model=\"simulated\", path=\"./output/simulated/undermodel/sim5/\")\n",
    "l.path\n",
    "htsl, p, m = l.load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test whether multi-individual load works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_loaddata(l_model=\"hdf5\", path=\"./output/simulated/TSI05s05e1/ch3_20cm/sim_ch\")\n",
    "l.iids = [\"iid0_A\", \"iid0_b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c45731409da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhtsl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e3128fd42277>\u001b[0m in \u001b[0;36mload_all_data\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0midx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_individual_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0midx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_individual_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_haplo_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e3128fd42277>\u001b[0m in \u001b[0;36mget_individual_idx\u001b[0;34m(self, f, iid, f_col)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sanity Check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "htsl, p, m = l.load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
