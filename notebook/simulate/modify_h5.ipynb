{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute-e-16-231.o2.rc.hms.harvard.edu\n",
      "HSM O2 Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/hapBLOCK\n"
     ]
    }
   ],
   "source": [
    "import allel\n",
    "import h5py  # Python Package to do the HDF5.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import socket\n",
    "import os\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "if socket_name == \"VioletQueen\":\n",
    "    path = \"/home/harald/git/HAPSBURG/\"   # The Path on Harald's machine\n",
    "elif socket_name[:7] == \"midway2\":\n",
    "    print(\"Midway jnovmbre partition detected.\")\n",
    "    path = \"/project2/jnovembre/hringbauer/git/hapBLOCK/\"  # The Path on Midway Cluster\n",
    "elif socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM O2 Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/hapBLOCK/\"  # The Path on Harvard Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd()) # Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifyHDF5Genotypes(object):\n",
    "    \"\"\"Class for Modifying HDF5 genotypes and\n",
    "    saving new HDF5s. Can downsample/throw down error/Create Readcound.\n",
    "    Plan: Also do contamination\"\"\"\n",
    "\n",
    "    f = 0    # The hdf5 object to modify\n",
    "    original_path = \"\" # Where to find the original HDF5\n",
    "    save_path = \"\"  # Where to save the modified HDF5 to\n",
    "    output = True # Whether to print any output\n",
    "    gt_new = []\n",
    "\n",
    "    def __init__(self, original_path=\"\", save_path=\"\", output=True):\n",
    "        \"\"\"pop_path: Where to load a HDF5 from\n",
    "           save_path: Where to save the new HDF5 to\"\"\"\n",
    "        self.output = output\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        if output == True:\n",
    "            print(\"Heyho back old friend. I started running\")\n",
    "        \n",
    "        if len(original_path)>0:\n",
    "            self.original_path = original_path\n",
    "            self.load_data()\n",
    "        else:\n",
    "            print(\"No HDF5 Loaded! Alarm. Alarm. Alarm.\")\n",
    "\n",
    "    def load_data(self, path=\"\"):\n",
    "        \"\"\"Load the HDF5 Data\"\"\"\n",
    "        if len(path)==0:\n",
    "            path = self.original_path\n",
    "        self.f = h5py.File(path, \"r\") # Load for Sanity Check. See below!\n",
    "        \n",
    "        if self.output == True:\n",
    "            print(\"Loaded HDF5\")\n",
    "            print(\"Loaded %i variants\" % np.shape(self.f[\"calldata/GT\"])[0])\n",
    "            print(\"Loaded %i individuals\" % np.shape(self.f[\"calldata/GT\"])[1])\n",
    "            print(list(self.f[\"calldata\"].keys()))\n",
    "            print(list(self.f[\"variants\"].keys()))\n",
    "            #self.f[\"samples\"] # Samples Vector\n",
    "        \n",
    "        ### Sanity Check whether both Genotypes are there and nothing else\n",
    "        assert(np.min(self.f[\"calldata/GT\"]) == 0)\n",
    "        assert(np.max(self.f[\"calldata/GT\"]) == 1)\n",
    "\n",
    "    def save_data(self, gt, ad, ref, alt, pos, \n",
    "                  rec, samples, path, gp=[],\n",
    "                  compression=\"gzip\", ad_group=True, gt_type=\"int8\"):\n",
    "        \"\"\"Create a new HDF5 File with Input Data.\n",
    "        gt: Genotype data [l,k,2]\n",
    "        ad: Allele depth [l,k,2]\n",
    "        ref: Reference Allele [l]\n",
    "        alt: Alternate Allele [l]\n",
    "        pos: Position  [l]\n",
    "        m: Map position [l]\n",
    "        samples: Sample IDs [k].\n",
    "        Save genotype data as int8, readcount data as int16.\n",
    "        ad: whether to save allele depth\n",
    "        gt_type: What genotype data type save\"\"\"\n",
    "\n",
    "        l, k, _ = np.shape(gt)  # Nr loci and Nr of Individuals\n",
    "\n",
    "        if os.path.exists(path):  # Do a Deletion of existing File there\n",
    "            os.remove(path)\n",
    "\n",
    "        dt = h5py.special_dtype(vlen=str)  # To have no problem with saving\n",
    "\n",
    "        with h5py.File(path, 'w') as f0:\n",
    "            ### Create all the Groups\n",
    "            f_map = f0.create_dataset(\"variants/MAP\", (l,), dtype='f')\n",
    "            if ad_group:\n",
    "                f_ad = f0.create_dataset(\"calldata/AD\", (l, k, 2), dtype='int8', compression=compression)\n",
    "            f_ref = f0.create_dataset(\"variants/REF\", (l,), dtype=dt)\n",
    "            f_alt = f0.create_dataset(\"variants/ALT\", (l,), dtype=dt)\n",
    "            f_pos = f0.create_dataset(\"variants/POS\", (l,), dtype='int32')\n",
    "            f_gt = f0.create_dataset(\"calldata/GT\", (l, k, 2), dtype=gt_type, compression=compression)\n",
    "            if len(gp)>0:\n",
    "                f_gp = f0.create_dataset(\"calldata/GP\", (l, k, 3), dtype=\"f\", compression=compression)     \n",
    "            f_samples = f0.create_dataset(\"samples\", (k,), dtype=dt)\n",
    "\n",
    "            ### Save the Data\n",
    "            f_map[:] = rec\n",
    "            if ad_group:\n",
    "                f_ad[:] = ad\n",
    "            f_ref[:] = ref.astype(\"S1\")\n",
    "            f_alt[:] = alt.astype(\"S1\")\n",
    "            f_pos[:] = pos\n",
    "            f_gt[:] = gt\n",
    "            if len(gp)>0:\n",
    "                f_gp[:] = gp\n",
    "            f_samples[:] = np.array(samples).astype(\"S10\")\n",
    "\n",
    "        if self.output == True:\n",
    "            print(f\"Successfully saved {k} individuals to: {path}\")\n",
    "\n",
    "    def create_error_gt(self, freq_flips=0.01):\n",
    "        \"\"\"Create Error on the HDF5 of genotypes.\n",
    "        freq_flips: How often to do flip of genotyps\"\"\"\n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        \n",
    "        switch = np.random.random(np.shape(gt)) < freq_flips\n",
    "        \n",
    "        if self.output == True:\n",
    "            print(f\"Swapping frac of SNPs: {np.mean(switch):.6f}\")\n",
    "\n",
    "        ### Switch the Genotypes\n",
    "        gt_new = (gt + switch) %2\n",
    "\n",
    "        self.save_data(gt_new, f[\"calldata/AD\"], f[\"variants/REF\"][:], f[\"variants/ALT\"][:], f[\"variants/POS\"], \n",
    "                       f[\"variants/MAP\"], f[\"samples\"][:], self.save_path)\n",
    "            \n",
    "    def downsample_gt(self, frac=0.9, ad=True, mult_alt=False, \n",
    "                      gt_type=\"int8\", compression=None):\n",
    "        \"\"\"Downsample the HDF5 to fewer reads.\n",
    "        Update also the recombination and position map if needed to remove missing values\n",
    "        frac: To what fraction of markers one downsamples\n",
    "        ad: Whether original HDF5 has AD field\n",
    "        mult_alt: Whether there are multiple alternative Allelels in the original HDF5\"\"\"\n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        \n",
    "        ### Decide on SNPs\n",
    "        l, n, _ = np.shape(gt)\n",
    "        survive = np.random.random(l) <= frac\n",
    "        print(f\"Fraction Loci surviving {np.mean(survive):.6f}\")\n",
    "        \n",
    "        ### Downsample\n",
    "        gt_new = gt[survive,:,:].astype(gt_type)\n",
    "        r_map_new = f[\"variants/MAP\"][survive]\n",
    "        if ad:\n",
    "            ad_new = f[\"calldata/AD\"][survive,:,:]\n",
    "        else:\n",
    "            ad_new = np.zeros(np.shape(gt_new), dtype=\"int8\")\n",
    "        \n",
    "        ref_new = f[\"variants/REF\"][survive]\n",
    "        \n",
    "        if mult_alt:\n",
    "            alt_new = f[\"variants/ALT\"][survive,0]   \n",
    "        else:\n",
    "            alt_new = f[\"variants/ALT\"][survive]\n",
    "        \n",
    "        pos_new = f[\"variants/POS\"][survive]\n",
    "        \n",
    "        ### Downsample where needed  \n",
    "        self.save_data(gt_new, ad_new, ref_new, alt_new, pos_new, r_map_new, \n",
    "                       f[\"samples\"], self.save_path, \n",
    "                       ad_group=ad, gt_type=gt_type, compression=compression)\n",
    "        \n",
    "    def generate_binomial_rc(self, mean_rc=1):\n",
    "        \"\"\"Generate Readcount Data from GT data.\n",
    "        mean_rc: The Mean total Readcount per site\"\"\"\n",
    "        \n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        \n",
    "        ### Create the Poisson Readcounts with the right mean\n",
    "        rc_full = poisson_readcounts(gt, mean_rc, output=self.output) \n",
    "        \n",
    "        self.save_data(gt, rc_full, f[\"variants/REF\"][:], f[\"variants/ALT\"][:], f[\"variants/POS\"], \n",
    "               f[\"variants/MAP\"], f[\"samples\"][:], self.save_path)\n",
    "        \n",
    "    def generate_lambda_rc(self, mean_rc = 1, norm_counts=True,\n",
    "                           lambda_path = \"./Data/1000Genomes/Coverage/mean_cov1240k_Marcus.csv\"):\n",
    "        \"\"\"Generate Readcount Data from GT data.\n",
    "        Use Table found at lambda_path for Lambdas \n",
    "        (relative. mean coverages, normed to 1 genome-wide)\n",
    "        norm_counts: Whether to normalize on overlapping Readcounts\"\"\"\n",
    "        \n",
    "        df_lambda = load_lambda(lambda_path, output=self.output)  ### Load the Lambda Data\n",
    "        \n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        l, n, _ = np.shape(gt)\n",
    "        \n",
    "        pos_f = f[\"variants/POS\"][:]  # The Position of the Original \n",
    "        _, i1, i2 = np.intersect1d(pos_f, df_lambda[\"Pos\"], return_indices=True)\n",
    "        \n",
    "        if self.output==True:\n",
    "            print(f\"Found {len(i1)} / {l} Loci in Lambda Table\")\n",
    "        \n",
    "        lambdas = df_lambda[\"Lambda\"].values[i2]\n",
    "        if norm_counts == True: # Normalize to extracted lambdas\n",
    "            lambdas = lambdas / np.mean(lambdas)\n",
    "            \n",
    "        mean_cov = lambdas * mean_rc  # Extract the Means that Intersect\n",
    "        gt = gt[i1,:,:]  # Downsample to Loci intersecting the Lambda Table \n",
    "        \n",
    "        ### Do the Binomial Readcount Sampling\n",
    "        rc_full = poisson_readcounts(gt, mean_cov[:,None], output=self.output) \n",
    "        \n",
    "        i1 = list(i1)  # So that it works with HDF5\n",
    "        self.save_data(gt, rc_full, f[\"variants/REF\"][i1], f[\"variants/ALT\"][i1], f[\"variants/POS\"][i1], \n",
    "               f[\"variants/MAP\"][i1], f[\"samples\"][:], self.save_path)\n",
    "        \n",
    "    def generate_ph(self, coverage = 1.0, error = 0.0):\n",
    "        \"\"\"Generate Pseudo-Haploid Data with fraction coverage sites covered,\n",
    "        and then error thrown down.\n",
    "        coverage: Fraction of sites covered\n",
    "        error: Fraction of sites with error. If >0, flip error added at random\"\"\"\n",
    "        \n",
    "        f = self.f\n",
    "        gt = f[\"calldata/GT\"]\n",
    "        l, _, _ = np.shape(gt)\n",
    "        \n",
    "        idx = np.random.random(l)<=coverage  # Which sites are covered\n",
    "        gt = gt[idx, :, :]  # Extract downsampled SNPs\n",
    "        \n",
    "        switch = [0,]\n",
    "        if error>0:\n",
    "            switch = (np.random.random(np.shape(gt)) < error) & (gt >= 0)\n",
    "            gt = (gt + switch) %2 # Switch the Genotypes\n",
    "                \n",
    "        if self.output:\n",
    "            print(f\"{np.sum(idx)} / {len(idx)} SNPs pseudohaploidized.\")\n",
    "            print(f\"Added fraction errors to SNPs: {np.mean(switch):.6f}\")\n",
    "            print(f\"Added sum errors: {np.sum(switch):.0f}\")\n",
    "        \n",
    "        rc = pseudo_haploid(gt) # Generate Pseudo-Haploid Readcounts\n",
    "        \n",
    "        idx = np.array(idx)  # So that it works with HDF5 (Boolean Indexing)\n",
    "        self.save_data(gt, rc, f[\"variants/REF\"][idx], f[\"variants/ALT\"][idx], f[\"variants/POS\"][idx], \n",
    "                       f[\"variants/MAP\"][idx], f[\"samples\"][:], self.save_path)\n",
    "        \n",
    "    def copy_rohinfo(self, load_path=\"\", save_path=\"\", file=\"roh_info.csv\"):\n",
    "        \"\"\"Copy in the ROH Info from folder of load path into folder of save_path.\n",
    "        file: Which file to copy (roh_info by default)\"\"\"\n",
    "        if len(load_path) == 0:\n",
    "            load_path = self.original_path\n",
    "            \n",
    "        if len(save_path) ==0 :\n",
    "            save_path = self.save_path\n",
    "            \n",
    "        save_path = os.path.dirname(save_path) + \"/\" + file\n",
    "        load_path = os.path.dirname(load_path) + \"/\" + file\n",
    "        \n",
    "        ### Copy the file\n",
    "        !cp $load_path $save_path  \n",
    "        \n",
    "##########################################\n",
    "#### Some Small Helper Functions\n",
    "\n",
    "def load_lambda(loadpath, ch=3, output=True):\n",
    "    \"\"\"Load and return the Lambda Vector\n",
    "    for Chromosome ch, and from path loadpath\"\"\"\n",
    "    df_lambda = pd.read_csv(loadpath)\n",
    "    mean = np.mean(df_lambda[\"Lambda\"])\n",
    "    assert(np.isclose(mean, 1))  # Sanity Check if Valid Lambda Vector\n",
    "    l=len(df_lambda)\n",
    "    df_lambda = df_lambda[df_lambda[\"Ch\"]==ch]\n",
    "    if output==True:\n",
    "        print(f\"Extracted {len(df_lambda)} / {l} Loci on Chr.{ch}\")\n",
    "    return df_lambda\n",
    "\n",
    "def poisson_readcounts(gt, mean_rc, output=True):\n",
    "    \"\"\"Create and return Poisson Readcount array.\n",
    "    gt: Underlying Genotype Matrix [l, n, 2]\n",
    "    Return readcound array: [l, n, 2]\"\"\"\n",
    "    l, n, _ = np.shape(gt)\n",
    "    rc_tot = np.random.poisson(lam=mean_rc, size = (l,n))  # Draw Full Readcounts\n",
    "\n",
    "    p = np.mean(gt, axis=2) # Get the Mean Allele Frequency per locus and individual\n",
    "    assert(np.max(p)<=1) ### Sanity Check whether allele freqs are right\n",
    "    assert(np.min(p)>=0)\n",
    "\n",
    "    rc_der = np.random.binomial(n=rc_tot, p=p)  # The derived Readcount (Binomial Sampling)\n",
    "    rc_ref = rc_tot - rc_der  # The Ref Readcount\n",
    "\n",
    "    rc_full = np.stack([rc_ref, rc_der], axis=2)\n",
    "    assert(np.shape(rc_full) == np.shape(gt))  # Check whether data was created properly\n",
    "\n",
    "    if output == True:\n",
    "        print(f\"Mean Readcount: {np.mean(rc_tot):.4f}\")\n",
    "    \n",
    "    return rc_full\n",
    "\n",
    "def pseudo_haploid(gt):\n",
    "    \"\"\"Create and return Pseudo-Haploid Readcount array\n",
    "    gt: Underlying Genotype Matrix [l, n, 2]\n",
    "    Return readcound array: [l, n, 2]\"\"\"\n",
    "    \n",
    "    p = np.mean(gt, axis=2) # Get the Mean Allele Frequency per locus and individual\n",
    "    \n",
    "    rc_der = np.random.binomial(n=1, p=p)  # The derived Readcount (Binomial Sampling)\n",
    "    rc_ref = 1 - rc_der  # The Ref Readcount\n",
    "    rc_full = np.stack([rc_ref, rc_der], axis=2)\n",
    "    \n",
    "    assert(np.max(rc_full)<=1)\n",
    "    assert(np.min(rc_full)==0)\n",
    "    assert(np.shape(rc_full) == np.shape(gt))  # Check whether data was created properly\n",
    "    return rc_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "org_folder = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240/chr\"\n",
    "out_folder = \"./Data/1000Genomes/HDF5/1240kHDF5/all1240bool0/chr\"\n",
    "\n",
    "chs = range(1, 23)\n",
    "\n",
    "for ch in chs:\n",
    "    load_path = org_folder + str(ch) + \".hdf5\"\n",
    "    save_path = out_folder + str(ch) + \".hdf5\" \n",
    "\n",
    "    # Make Directory if not already there\n",
    "    if not os.path.exists(os.path.dirname(save_path)):   \n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "    #os.remove(save_path)  # For previous whoopsie\n",
    "    m = ModifyHDF5Genotypes(original_path=load_path, save_path=save_path)\n",
    "    m.downsample_gt(frac=1.0, ad=False, mult_alt=True, gt_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "Loaded 77652 variants\n",
      "Loaded 6 individuals\n",
      "['AD', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n"
     ]
    }
   ],
   "source": [
    "path = \"/n/groups/reich/hringbauer/git/hapBLOCK/output/simulated/TSI/ch3_8cm/sim_ch3.h5\"\n",
    "\n",
    "f = h5py.File(path, \"r\") # Load for Sanity Check. See below!\n",
    "        \n",
    "print(\"Loaded HDF5\")\n",
    "print(\"Loaded %i variants\" % np.shape(f[\"calldata/AD\"])[0])\n",
    "print(\"Loaded %i individuals\" % np.shape(f[\"calldata/AD\"])[1])\n",
    "print(list(f[\"calldata\"].keys()))\n",
    "print(list(f[\"variants\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = f[\"calldata/GT\"][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_from_gts(gts):\n",
    "    \"\"\"Create GP [l,k,3] from\n",
    "    genotypes [l,k,2], with prob.\n",
    "    of genotype set to 1\"\"\"\n",
    "    gs = np.sum(gts, axis=2)\n",
    "    l, k = np.shape(gs)\n",
    "\n",
    "    gp = np.zeros((l,k,3), dtype=\"f\")\n",
    "    gp[gs==0,0]=1\n",
    "    gp[gs==1,1]=1\n",
    "    gp[gs==2,2]=1\n",
    "    #(np.sum(gp, axis=2)==1).all()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = gp_from_gts(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
